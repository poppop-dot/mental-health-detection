import os
import torch
import torch.nn.functional as F
from contextlib import asynccontextmanager
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# ================= é…ç½®åŒºåŸŸ =================
# æŒ‡å®šæ˜¾å¡ï¼Œä¿æŒå•å¡è¿è¡Œ
os.environ["CUDA_VISIBLE_DEVICES"] = "0"

MODEL_PATH = "./mentalbert_finetuned_final/final_model"

# å®šä¹‰è¾“å…¥æ•°æ®æ ¼å¼ (Request Body)
class SentimentRequest(BaseModel):
    text: str

# å®šä¹‰è¾“å‡ºæ•°æ®æ ¼å¼ (Response Body)
class SentimentResponse(BaseModel):
    label: str
    risk_score: float
    probabilities: dict

# ================= å…¨å±€å˜é‡ =================
# ç”¨äºå­˜å‚¨åŠ è½½åçš„æ¨¡å‹ï¼Œé¿å…åå¤åŠ è½½
ml_models = {}

# ================= 1. ç”Ÿå‘½å‘¨æœŸç®¡ç† (Lifespan) =================
# è¿™æ˜¯ FastAPI æ¨èçš„ç°ä»£å†™æ³•ï¼šåœ¨æœåŠ¡å¯åŠ¨å‰åŠ è½½æ¨¡å‹ï¼ŒæœåŠ¡å…³é—­åæ¸…ç†
@asynccontextmanager
async def lifespan(app: FastAPI):
    print(f"ğŸš€ [Startup] æ­£åœ¨åŠ è½½æ¨¡å‹: {MODEL_PATH} ...")
    try:
        device = "cuda" if torch.cuda.is_available() else "cpu"
        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
        model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)
        model.to(device)
        model.eval() # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
        
        # å°†æ¨¡å‹å’Œé…ç½®å­˜å…¥å…¨å±€å­—å…¸
        ml_models["tokenizer"] = tokenizer
        ml_models["model"] = model
        ml_models["device"] = device
        ml_models["labels"] = {0: "Healthy", 1: "Risk"}
        
        print(f"[Startup] æ¨¡å‹åŠ è½½å®Œæˆï¼è¿è¡Œè®¾å¤‡: {device}")
        yield
        
    except Exception as e:
        print(f"[Error] æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        # è¿™é‡Œå¯ä»¥è®©ç¨‹åºé€€å‡ºï¼Œæˆ–è€…è®°å½•æ—¥å¿—
    finally:
        print("[Shutdown] æœåŠ¡æ­£åœ¨å…³é—­ï¼Œæ¸…ç†èµ„æº...")
        ml_models.clear()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

# ================= 2. åˆå§‹åŒ– App =================
app = FastAPI(
    title="MentalBERT API Service",
    description="åŸºäºå¾®è°ƒ MentalBERT çš„å¿ƒç†å¥åº·é£é™©æ£€æµ‹ API",
    version="1.0.0",
    lifespan=lifespan
)

# ================= 3. æ ¸å¿ƒé¢„æµ‹æ¥å£ =================
@app.post("/predict", response_model=SentimentResponse)
async def predict(request: SentimentRequest):
    # 1. è·å–æ¨¡å‹èµ„æº
    if "model" not in ml_models:
        raise HTTPException(status_code=500, detail="Model not initialized")
    
    tokenizer = ml_models["tokenizer"]
    model = ml_models["model"]
    device = ml_models["device"]
    id2label = ml_models["labels"]

    # 2. æ–‡æœ¬é¢„å¤„ç†
    if not request.text.strip():
        raise HTTPException(status_code=400, detail="Input text cannot be empty")

    inputs = tokenizer(
        request.text, 
        return_tensors="pt", 
        truncation=True, 
        max_length=512,
        padding=True
    ).to(device)

    # 3. æ¨¡å‹æ¨ç†
    with torch.no_grad():
        outputs = model(**inputs)
        # è½¬ä¸ºæ¦‚ç‡
        probs = F.softmax(outputs.logits, dim=-1)[0]
        
        # è·å–æœ€å¤§æ¦‚ç‡çš„æ ‡ç­¾
        pred_idx = torch.argmax(probs).item()
        pred_label = id2label[pred_idx]
        
        # æå–é£é™©æ¦‚ç‡ (Label 1 çš„æ¦‚ç‡)
        risk_score = float(probs[1])

    # 4. æ„é€ è¿”å›ç»“æœ
    return SentimentResponse(
        label=pred_label,
        risk_score=round(risk_score, 4),
        probabilities={
            "Healthy": float(probs[0]),
            "Risk": float(probs[1])
        }
    )

# ================= 4. å¥åº·æ£€æŸ¥æ¥å£ =================
@app.get("/health")
async def health_check():
    return {"status": "ok", "device": ml_models.get("device", "unknown")}

# ================= å¯åŠ¨å…¥å£ =================
if __name__ == "__main__":
    import uvicorn
    # host="0.0.0.0" å…è®¸å±€åŸŸç½‘è®¿é—®
    # port=8000 æ˜¯æ ‡å‡†ç«¯å£
    uvicorn.run(app, host="0.0.0.0", port=8060)